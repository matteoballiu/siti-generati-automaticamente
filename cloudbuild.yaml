# Questo è il file cloudbuild.yaml COMPLETO e CORRETTO, progettato per funzionare con la massima robustezza.

# Specifica l'account di servizio che eseguirà il build.
# Questo account deve avere i permessi per accedere a Secret Manager, Pub/Sub e Storage.
serviceAccount: projects/generatore-siti-nuovo/serviceAccounts/138602713462@cloudbuild.gserviceaccount.com
# Specifica il bucket dove verranno archiviati i log del build.
logsBucket: gs://generatore-siti-build-logs-138602713462/

# availableSecrets è stato rimosso dal top-level per semplificare la gestione dei segreti.
# I segreti verranno recuperati ed esportati direttamente all'interno degli step che ne hanno bisogno.

steps:
# Step 0: Clonare il repository GitHub
# Cloud Build clona già il repository specificato nel trigger all'inizio del build.
# Questo step è qui solo per riferimento se avessi bisogno di clonare un altro repository
# o se il trigger Pub/Sub non clona il repo principale (comportamento che varia).
# Per massima robustezza, lo manteniamo in una forma semplice.
- name: 'gcr.io/cloud-builders/git'
  id: Clone-Repository
  args:
    - 'clone'
    - '--depth=1'
    - 'https://github.com/matteoballiu/siti-generati-automaticamente'
    - '.' # Clona nella directory corrente di lavoro del build

# RIMOSSO: Step 0 (ex Step 1): Install-Dependencies
# Le dipendenze Python verranno ora installate direttamente nello step Generate-Website.

# Step 0 (ex Step 1): Esegui lo script di generazione del sito, iniettando i segreti e usando gsutil/gcloud
# Questo step usa l'immagine gcloud che include python, gcloud e gsutil.
- name: 'gcr.io/cloud-builders/gcloud-slim' # Immagine gcloud-slim che include Python, gcloud, gsutil
  id: Generate-Website
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Installiamo le dipendenze Python direttamente in questo step
      pip install --upgrade pip # Aggiorna pip per evitare warning
      pip install --no-cache-dir -r requirements.txt 
      pip install --no-cache-dir google-cloud-storage 

      # Recupera i segreti da Secret Manager e li esporta come variabili d'ambiente.
      export OPENAI_API_KEY=$(gcloud secrets versions access latest --secret=OPENAI_API_KEY --project=${PROJECT_ID})
      export PEXELS_API_KEY=$(gcloud secrets versions access latest --secret=PEXELS_API_KEY --project=${PROJECT_ID})
      export IMAP_USER=$(gcloud secrets versions access latest --secret=IMAP_USER --project=${PROJECT_ID})
      export IMAP_PASSWORD=$(gcloud secrets versions access latest --secret=IMAP_PASSWORD --project=${PROJECT_ID})
      export IMAP_HOST=$(gcloud secrets versions access latest --secret=IMAP_HOST --project=${PROJECT_ID})
      
      # Scarica il file JSON dell'email dal bucket di input
      gsutil -q cp gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME} ./input_email.json || \
      (echo "ERRORE: File di input JSON non trovato nel bucket: gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME}" && exit 1)
      
      # Verifica che le variabili segrete siano state impostate (non stampa i valori)
      echo "Verifica segreti: OPENAI_API_KEY=${OPENAI_API_KEY:+Set}, PEXELS_API_KEY=${PEXELS_API_KEY:+Set}, IMAP_USER=${IMAP_USER:+Set}, IMAP_PASSWORD=${IMAP_PASSWORD:+Set}, IMAP_HOST=${IMAP_HOST:+Set}"
      
      echo "Eseguo run_automation.py con input_email.json..."
      python3 run_automation.py ./input_email.json
  env:
    - 'PYTHONUSERBASE=/usr/local' # Mantenuto per coerenza, ma l'installazione è globale nello step

# Step 1 (ex Step 2): Carica i file generati sul bucket di hosting statico
- name: 'gcr.io/cloud-builders/gsutil'
  id: Upload-Website
  args: ['-m', 'cp', '-r', './public/*', 'gs://${_HOSTING_BUCKET_NAME}/']

# Step 2 (ex Step 3): Pulisci i file JSON di input dal bucket dopo l'elaborazione (Opzionale ma buona pratica)
- name: 'gcr.io/cloud-builders/gsutil'
  id: Clean-Input-File
  args: ['rm', 'gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME}']
  waitFor: ["Upload-Website"]

timeout: 1800s # Aumenta il timeout a 30 minuti (1800 secondi) se le generazioni AI sono lunghe