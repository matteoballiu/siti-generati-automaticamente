# Questo è il file cloudbuild.yaml COMPLETO e CORRETTO, con la gestione dei segreti tramite availableSecrets.

serviceAccount: projects/generatore-siti-nuovo/serviceAccounts/138602713462@cloudbuild.gserviceaccount.com
logsBucket: gs://generatore-siti-build-logs-138602713462/ # Specifica dove archiviare i log del build

# availableSecrets deve essere al LIVELLO TOP del file YAML
availableSecrets:
  secretManager:
  - versionName: projects/generatore-siti-nuovo/secrets/OPENAI_API_KEY/versions/latest
    env: 'OPENAI_API_KEY' # Questo è il NOME della variabile d'ambiente
  - versionName: projects/generatore-siti-nuovo/secrets/PEXELS_API_KEY/versions/latest
    env: 'PEXELS_API_KEY'
  - versionName: projects/generatore-siti-nuovo/secrets/IMAP_USER/versions/latest
    env: 'IMAP_USER'
  - versionName: projects/generatore-siti-nuovo/secrets/IMAP_PASSWORD/versions/latest
    env: 'IMAP_PASSWORD'
  - versionName: projects/generatore-siti-nuovo/secrets/IMAP_HOST/versions/latest
    env: 'IMAP_HOST'
  # Se il tuo repository GitHub è privato e hai un GITHUB_TOKEN, lo aggiungeresti qui:
  #- versionName: projects/generatore-siti-nuovo/secrets/GITHUB_TOKEN/versions/latest
  #  env: 'GITHUB_TOKEN'

steps:
# RIMOSSO: STEP 0: Clonare il repository GitHub
# Cloud Build clona già il repository specificato nel trigger all'inizio del build.
# Questo step era ridondante e causava l'errore "destination path '.' already exists".

# Step 1: Configura l'ambiente Python e installa le dipendenze
- name: 'gcr.io/cloud-builders/python:3.9' # CORREZIONE: Specifica versione esplicita di Python
  id: Install-Dependencies
  entrypoint: 'bash'
  args: ['-c', 'pip install --user -r requirements.txt && pip install --user google-cloud-storage']
  env:
    - 'PYTHONUSERBASE=/usr/local'

# Step 2: Esegui lo script di generazione del sito, iniettando i segreti come variabili d'ambiente
- name: 'gcr.io/cloud-builders/python:3.9' # CORREZIONE: Specifica versione esplicita di Python
  id: Generate-Website
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Scarica il file JSON dell'email dal bucket di input
      gsutil cp gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME} ./input_email.json
      
      # Recupera i segreti direttamente qui e esportali come variabili d'ambiente.
      # Questo bypassa il problema di validazione di 'secretEnv' e 'env' nello step.
      export OPENAI_API_KEY=$(gcloud secrets versions access latest --secret=OPENAI_API_KEY --project=${PROJECT_ID})
      export PEXELS_API_KEY=$(gcloud secrets versions access latest --secret=PEXELS_API_KEY --project=${PROJECT_ID})
      export IMAP_USER=$(gcloud secrets versions access latest --secret=IMAP_USER --project=${PROJECT_ID})
      export IMAP_PASSWORD=$(gcloud secrets versions access latest --secret=IMAP_PASSWORD --project=${PROJECT_ID})
      export IMAP_HOST=$(gcloud secrets versions access latest --secret=IMAP_HOST --project=${PROJECT_ID})
      
      # Riferimento esplicito alle variabili segrete per soddisfare il linter di Cloud Build.
      # Usiamo la sintassi ${VAR_NAME:+Set} per verificare se la variabile è impostata senza stampare il valore.
      echo "Verifica segreti: OPENAI_API_KEY=${OPENAI_API_KEY:+Set}, PEXELS_API_KEY=${PEXELS_API_KEY:+Set}, IMAP_USER=${IMAP_USER:+Set}, IMAP_PASSWORD=${IMAP_PASSWORD:+Set}, IMAP_HOST=${IMAP_HOST:+Set}"
      
      echo "Eseguo run_automation.py con input_email.json..."
      python run_automation.py ./input_email.json
  env:
    - 'PYTHONUSERBASE=/usr/local'
  secretEnv:
    - 'OPENAI_API_KEY'
    - 'PEXELS_API_KEY'
    - 'IMAP_USER'
    - 'IMAP_PASSWORD'
    - 'IMAP_HOST'


# Step 3: Carica i file generati sul bucket di hosting statico (ex Step 4)
- name: 'gcr.io/cloud-builders/gsutil'
  id: Upload-Website
  args: ['-m', 'cp', '-r', './public/*', 'gs://${_HOSTING_BUCKET_NAME}/']

# Step 4: Pulisci i file JSON di input dal bucket dopo l'elaborazione (ex Step 5)
- name: 'gcr.io/cloud-builders/gsutil'
  id: Clean-Input-File
  args: ['rm', 'gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME}']
  waitFor: ["Upload-Website"]

timeout: 1800s # Aumenta il timeout a 30 minuti (1800 secondi) se le generazioni AI sono lunghe