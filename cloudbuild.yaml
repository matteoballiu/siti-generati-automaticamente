# Questo è il file cloudbuild.yaml COMPLETO e CORRETTO, progettato per funzionare con la massima robustezza.

# Specifica l'account di servizio che eseguirà il build.
# Questo account deve avere i permessi per accedere a Secret Manager, Pub/Sub e Storage.
serviceAccount: projects/generatore-siti-nuovo/serviceAccounts/138602713462@cloudbuild.gserviceaccount.com
# Specifica il bucket dove verranno archiviati i log del build.
logsBucket: gs://generatore-siti-build-logs-138602713462/

# availableSecrets è stato rimosso dal top-level per semplificare la gestione dei segreti.
# I segreti verranno recuperati ed esportati direttamente all'interno degli step che ne hanno bisogno.

steps:
# Step 0: Clonare il repository GitHub
# Cloud Build clona già il repository specificato nel trigger all'inizio del build.
# Questo step è qui solo per riferimento se avessi bisogno di clonare un altro repository
# o se il trigger Pub/Sub non clona il repo principale (comportamento che varia).
# Per massima robustezza, lo manteniamo in una forma semplice.
- name: 'gcr.io/cloud-builders/git'
  id: Clone-Repository
  args:
    - 'clone'
    - '--depth=1'
    - 'https://github.com/matteoballiu/siti-generati-automaticamente'
    - '.' # Clona nella directory corrente di lavoro del build

# Step 1: Configura l'ambiente Python e installa le dipendenze
# Utilizza un'immagine Python specifica e affidabile.
- name: 'gcr.io/cloud-builders/python/debian:python3.9'
  id: Install-Dependencies
  entrypoint: 'bash'
  args: ['-c', 'pip install --user -r requirements.txt && pip install --user google-cloud-storage']
  env:
    - 'PYTHONUSERBASE=/usr/local' # Assicura che le librerie siano accessibili nel PATH

# Step 2: Esegui lo script di generazione del sito, iniettando i segreti direttamente
# Questo step recupera i segreti da Secret Manager e li esporta come variabili d'ambiente.
- name: 'gcr.io/cloud-builders/python/debian:python3.9'
  id: Generate-Website
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Recupera i segreti da Secret Manager e li esporta come variabili d'ambiente.
      # Questo è il metodo più robusto per Cloud Build.
      export OPENAI_API_KEY=$(gcloud secrets versions access latest --secret=OPENAI_API_KEY --project=${PROJECT_ID})
      export PEXELS_API_KEY=$(gcloud secrets versions access latest --secret=PEXELS_API_KEY --project=${PROJECT_ID})
      export IMAP_USER=$(gcloud secrets versions access latest --secret=IMAP_USER --project=${PROJECT_ID})
      export IMAP_PASSWORD=$(gcloud secrets versions access latest --secret=IMAP_PASSWORD --project=${PROJECT_ID})
      export IMAP_HOST=$(gcloud secrets versions access latest --secret=IMAP_HOST --project=${PROJECT_ID})
      
      # Scarica il file JSON dell'email dal bucket di input
      gsutil cp gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME} ./input_email.json
      
      # Verifica che le variabili segrete siano state impostate (non stampa i valori)
      echo "Verifica segreti: OPENAI_API_KEY=${OPENAI_API_KEY:+Set}, PEXELS_API_KEY=${PEXELS_API_KEY:+Set}, IMAP_USER=${IMAP_USER:+Set}, IMAP_PASSWORD=${IMAP_PASSWORD:+Set}, IMAP_HOST=${IMAP_HOST:+Set}"
      
      echo "Eseguo run_automation.py con input_email.json..."
      python run_automation.py ./input_email.json
  env:
    - 'PYTHONUSERBASE=/usr/local' # Assicura che le librerie installate siano nel PATH
  # I blocchi 'availableSecrets' e 'secretEnv' sono stati rimossi da questo step.


# Step 3: Carica i file generati sul bucket di hosting statico
- name: 'gcr.io/cloud-builders/gsutil'
  id: Upload-Website
  args: ['-m', 'cp', '-r', './public/*', 'gs://${_HOSTING_BUCKET_NAME}/']

# Step 4: Pulisci i file JSON di input dal bucket dopo l'elaborazione (Opzionale ma buona pratica)
- name: 'gcr.io/cloud-builders/gsutil'
  id: Clean-Input-File
  args: ['rm', 'gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME}']
  waitFor: ["Upload-Website"]

timeout: 1800s # Aumenta il timeout a 30 minuti (1800 secondi) se le generazioni AI sono lunghe