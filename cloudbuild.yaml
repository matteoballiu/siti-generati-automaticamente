# Questo è il file cloudbuild.yaml COMPLETO e CORRETTO, progettato per funzionare con la massima robustezza.

# Specifica l'account di servizio che eseguirà il build.
# Questo account deve avere i permessi per accedere a Secret Manager, Pub/Sub e Storage.
serviceAccount: projects/generatore-siti-nuovo/serviceAccounts/138602713462@cloudbuild.gserviceaccount.com
# Specifica il bucket dove verranno archiviati i log del build.
logsBucket: gs://generatore-siti-build-logs-138602713462/

# availableSecrets è stato rimosso dal top-level per semplificare la gestione dei segreti.
# I segreti verranno recuperati ed esportati direttamente all'interno degli step che ne hanno bisogno.

steps:
# RIMOSSO: Step 0: Clonare il repository GitHub
# Cloud Build clona automaticamente il repository specificato nel trigger all'inizio del build.
# Questo step era ridondante e causava l'errore "destination path '.' already exists".

# Step 0 (ex Step 1): Configura l'ambiente Python e installa le dipendenze
# Utilizza un'immagine Python specifica e affidabile.
# Installiamo le dipendenze in un ambiente persistente per gli step successivi.
- name: 'python:3.9' # Immagine Python ufficiale da Docker Hub
  id: Install-Dependencies
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      pip install --upgrade pip # Aggiorna pip per evitare warning
      pip install --no-cache-dir -r requirements.txt # Installa in ambiente standard
      pip install --no-cache-dir google-cloud-storage # Installa in ambiente standard
  # Rimosso env: PYTHONUSERBASE=/usr/local per installare direttamente nell'ambiente dell'immagine

# Step 1 (ex Step 2): Esegui lo script di generazione del sito, iniettando i segreti e usando gsutil/gcloud
# Questo step usa l'immagine gcloud che include python, gcloud e gsutil.
- name: 'gcr.io/cloud-builders/gcloud-slim' # Immagine gcloud-slim che include Python, gcloud, gsutil
  id: Generate-Website
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      # Recupera i segreti da Secret Manager e li esporta come variabili d'ambiente.
      export OPENAI_API_KEY=$(gcloud secrets versions access latest --secret=OPENAI_API_KEY --project=${PROJECT_ID})
      export PEXELS_API_KEY=$(gcloud secrets versions access latest --secret=PEXELS_API_KEY --project=${PROJECT_ID})
      export IMAP_USER=$(gcloud secrets versions access latest --secret=IMAP_USER --project=${PROJECT_ID})
      export IMAP_PASSWORD=$(gcloud secrets versions access latest --secret=IMAP_PASSWORD --project=${PROJECT_ID})
      export IMAP_HOST=$(gcloud secrets versions access latest --secret=IMAP_HOST --project=${PROJECT_ID})
      
      # Scarica il file JSON dell'email dal bucket di input
      gsutil cp gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME} ./input_email.json
      
      # Verifica che le variabili segrete siano state impostate (non stampa i valori)
      echo "Verifica segreti: OPENAI_API_KEY=${OPENAI_API_KEY:+Set}, PEXELS_API_KEY=${PEXELS_API_KEY:+Set}, IMAP_USER=${IMAP_USER:+Set}, IMAP_PASSWORD=${IMAP_PASSWORD:+Set}, IMAP_HOST=${IMAP_HOST:+Set}"
      
      echo "Eseguo run_automation.py con input_email.json..."
      python run_automation.py ./input_email.json
  # Rimosso env: PYTHONUSERBASE=/usr/local qui. Le dipendenze sono installate globalmente nello step precedente.


# Step 2 (ex Step 3): Carica i file generati sul bucket di hosting statico
- name: 'gcr.io/cloud-builders/gsutil'
  id: Upload-Website
  args: ['-m', 'cp', '-r', './public/*', 'gs://${_HOSTING_BUCKET_NAME}/']

# Step 3 (ex Step 4): Pulisci i file JSON di input dal bucket dopo l'elaborazione (Opzionale ma buona pratica)
- name: 'gcr.io/cloud-builders/gsutil'
  id: Clean-Input-File
  args: ['rm', 'gs://${_INPUT_BUCKET_NAME}/${_INPUT_FILE_NAME}']
  waitFor: ["Upload-Website"]

timeout: 1800s # Aumenta il timeout a 30 minuti (1800 secondi) se le generazioni AI sono lunghe